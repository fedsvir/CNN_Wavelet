{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d3aebc7",
   "metadata": {},
   "source": [
    "**Author**: Fedor Sviridov\n",
    "\n",
    "This document presents the results of an artificial intelligence project.\n",
    "\n",
    "**Abstract**.\n",
    "\n",
    "**Objective**.  \n",
    "To attempt to solve a clustering problem for a large set of gamma-ray burst light curves: to apply a wavelet transform to them, extract features from the resulting scalograms using a pre-trained convolutional neural network, and finally apply clustering algorithms for high-dimensional data.\n",
    "\n",
    "**Data Preparation**.  \n",
    "I downloaded gamma-ray burst data recorded by the Fermi/GBM telescope from January 1, 2020 to the present. During this period, approximately 1060 bursts were detected. From the TTE data, light curves were constructed with a time resolution of 0.128 seconds in the energy range of 50–300 keV. The time interval for all light curves was chosen from −100 s to 400 s relative to the trigger time. Light curves that did not contain data covering this time interval were discarded. _Min–max_ scaling was applied to the light curves.\n",
    "\n",
    "**Wavelet Transform**.  \n",
    "For the wavelet analysis, I used the PyWavelets module. The mother wavelet was chosen as ‘cmor1.5-1.0’, with a bandwidth of 1.5 and a central frequency of 1.0. All normalized light curves were transformed using the same set of scales, corresponding to a frequency range from 10⁻⁴ Hz to 9.5 Hz. The result of the transform is a matrix of wavelet power spectrum coefficients (|w|²). These matrices were also normalized using _min–max_ scaling.\n",
    "\n",
    "**Convolutional Neural Network**.  \n",
    "To analyze the resulting scalogram images, CLIP was used—a pre-trained CNN whose purpose is to associate any image with a text description of that image (https://github.com/openai/CLIP.git).  \n",
    "The input image must be three-channel with a resolution of 224 × 224; therefore, the scalogram matrices were resized to this resolution and replicated across three channels.  \n",
    "As a result, features were extracted from approximately 1000 normalized scalograms, each feature vector having a dimensionality of 512.\n",
    "\n",
    "**Dimensionality Reduction**.  \n",
    "I attempted to include additional light-curve parameters such as duration, spectral characteristics, luminosity, and redshift, but this did not provide significant additional information and did not strongly affect the final clustering. Therefore, I restricted the analysis to parameters obtained solely from the scalogram images.\n",
    "\n",
    "To reduce the dimensionality of the resulting data, I applied principal component analysis (PCA), retaining 30 components, which corresponds to approximately 95% of the total variance.\n",
    "\n",
    "**Clustering**.  \n",
    "For clustering, I used DBSCAN, with the parameter *eps* determined by constructing the k-nearest-neighbor distance plot. To visualize the DBSCAN results, I used the t-SNE algorithm; this method was also employed as an alternative approach for identifying clusters. The obtained results and conclusions are presented below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9573ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main module\n",
    "import numpy as np\n",
    "import pywt\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "#import clip\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "def wavelet(signal, time, scales, wt=\"cmor1.5-1.0\"):\n",
    "    \"\"\"\n",
    "    Method to perform wavelet transformation (CWT)\n",
    "    Parameters:\n",
    "        signal (array) - signal values\n",
    "        time (array) - array of time points (used to calculate sampling period)\n",
    "        scales (array) - array of wavelet scales\n",
    "        wt (str) - name of the mother wavelet\n",
    "    Returns:\n",
    "        (array) 2D array of wavelet power spectrum coefficients (|w|^2)\n",
    "    \"\"\"\n",
    "    # logarithmic scale for scales, as suggested by Torrence & Compo:\n",
    "    sampling_period = np.diff(time).mean()\n",
    "    # frequency = scales**-1/sampling_period\n",
    "    cwt_coeff, freqs = pywt.cwt(signal, scales, wt, sampling_period=sampling_period)\n",
    "\n",
    "    # take the squeared absolute value of complex result and remove the bounds\n",
    "    return (np.abs(cwt_coeff[:-1, :-1])**2, freqs)\n",
    "\n",
    "def get_features(cwt_coeff):\n",
    "    \"\"\"\n",
    "    To get features from wavelet images using CLIP\n",
    "    https://github.com/openai/CLIP.git\n",
    "    Parameters:\n",
    "        cwt_coeff (array) - 2D array of any size (M, N)\n",
    "    Returns:\n",
    "        (1D array) 512 feauters of the image\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "    # min-max normalization\n",
    "    mn = np.min(cwt_coeff)\n",
    "    mx = np.max(cwt_coeff)\n",
    "    cwt_coeff = (cwt_coeff - mn)/(mx - mn)\n",
    "\n",
    "    #convert to torch.tensor\n",
    "    cwt_coeff_tensor = torch.from_numpy(cwt_coeff).unsqueeze(0).unsqueeze(0) #shape: [1, 1, H, W]\n",
    "    # Resize [1, 1, H, W] -> [1,1,224,224]\n",
    "    cwt_coeff_tensor = F.interpolate(cwt_coeff_tensor, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "    # Repeat the channel 3 times: [1,1,224,224] -> [1,3,224,224]\n",
    "    cwt_coeff_tensor = cwt_coeff_tensor.repeat(1, 3, 1, 1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(cwt_coeff_tensor)\n",
    "    return image_features\n",
    "\n",
    "def pca(features, n_components=50):\n",
    "    \"\"\"\n",
    "    PCA algorithm\n",
    "    Parameters:\n",
    "        features (2D array) - sample of image feaures\n",
    "        n_components (int) - final dimension\n",
    "    Returns:\n",
    "        print value of saved information\n",
    "        (array) reduced features\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_components)\n",
    "    features_reduced = pca.fit_transform(features)\n",
    "    total_variance_retained = np.sum(pca.explained_variance_ratio_)\n",
    "    print(f'total_variance_retained = {total_variance_retained}')\n",
    "\n",
    "    return features_reduced\n",
    "\n",
    "def dbscan(red_features, eps=3, min_samples = 5):\n",
    "    \"\"\"\n",
    "    The DBSCAN algorithm for custering\n",
    "    Parameters:\n",
    "        red_features (array) - input data\n",
    "        eps (float) - maximum distance between two samples\n",
    "                        for one to be considered as in the neighborhood of the other\n",
    "        min_samples (int) - number of samples (or total weight) in a neighborhood\n",
    "                            for a point to be considered as a core point\n",
    "    Returns:\n",
    "        (array) labels of the points\n",
    "    \"\"\"\n",
    "    labels = DBSCAN(eps=eps, min_samples=min_samples).fit_predict(red_features)\n",
    "    return labels\n",
    "\n",
    "def t_SNE(features, dimension=2, perplexity=20, learning_rate=200):\n",
    "    \"\"\"\n",
    "    The method to visualize high-dimensional data\n",
    "    Parameters:\n",
    "        features (array) - input data\n",
    "        dimension (int) - dimension of the embedded space\n",
    "        perplexity (float) - is related to the number of nearest neighbors.\n",
    "                            Consider selecting a value between 5 and 50\n",
    "        learning_rate (float) - for t-SNE is usually in the range [10.0, 1000.0]\n",
    "    \"\"\"\n",
    "    tsne = TSNE(\n",
    "    n_components = dimension,\n",
    "    perplexity=perplexity,\n",
    "    learning_rate=learning_rate,\n",
    "    n_iter=1000,\n",
    "    random_state=42)\n",
    "\n",
    "    return tsne.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf65c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./features.h5', 'r') as file:\n",
    "    names = np.array(list(file.keys()))\n",
    "    problem_names = np.array(file.attrs['error_name'])\n",
    "\n",
    "#remove GRB names having problems\n",
    "names = names[~np.isin(names, problem_names)]\n",
    "# get features for certain names\n",
    "with h5py.File('./features.h5', 'r') as file:\n",
    "    features = [file[f'{name}/features'][:] for name in names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ecd00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "red_features = pca(features, n_components=30)\n",
    "del features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee2257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we plot the k-distance graph to determine the optimal value \n",
    "for the eps parameter in the DBSCAN algorithm.\n",
    "\"\"\"\n",
    "def k_dist(features, min_samples):\n",
    "    k = min_samples - 1\n",
    "\n",
    "    neighbors = NearestNeighbors(n_neighbors=k)\n",
    "    neighbors_fit = neighbors.fit(red_features)\n",
    "    distances, indices = neighbors_fit.kneighbors(red_features)\n",
    "\n",
    "    k_distances = distances[:, k-1]\n",
    "    k_distances = np.sort(k_distances)\n",
    "\n",
    "    return k_distances\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "for min_samples in range(5, 40, 5):\n",
    "    k_distances = k_dist(red_features, min_samples)\n",
    "    plt.plot(k_distances, label=f'k= {min_samples - 1}')\n",
    "plt.xlabel('Points sorted by distance')\n",
    "plt.ylabel(f'Distance to k-th nearest neighbor')\n",
    "plt.title('k-distance graph to choose eps')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "#plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e6939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN clastering\n",
    "eps = 0.8\n",
    "min_samples = 10\n",
    "\n",
    "labels = dbscan(red_features ,eps=eps, min_samples=min_samples)\n",
    "\n",
    "classes  = {str(lb): names[labels == lb] for lb in set(labels)}\n",
    "n = {lb: len(classes[lb]) for lb in classes.keys()}\n",
    "\n",
    "# t-SNE visualization\n",
    "perplexity = 20\n",
    "features_2d = t_SNE(red_features, perplexity=perplexity, learning_rate=200)\n",
    "\n",
    "unique_labels = np.unique(labels)\n",
    "plt.figure(figsize=(10,5))\n",
    "sc = plt.scatter(features_2d[:,0], features_2d[:,1], c=labels, alpha=0.7)\n",
    "plt.title(f'DBSCAN clustering, classes = {n}')\n",
    "cbar = plt.colorbar(sc, ticks=unique_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4174d4",
   "metadata": {},
   "source": [
    "I was unable to obtain any interesting classes/clusters based on the selected data. The DBSCAN algorithm identifies only a small number of bursts (outliers) – cluster label 1 (the yellow class). This class contains just 15 bursts, and almost all of them have short durations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./duration.h5', 'r') as file:\n",
    "    duration = np.array(file['duration'])\n",
    "\n",
    "mask = np.isin(names, classes['1'])\n",
    "unusual_grbs = names[mask]\n",
    "print(f'Duration of GRBs from 1-st class: {duration[np.isin(names, unusual_grbs)]}')\n",
    "print(f'Mean duration of 1-st class: {duration[np.isin(names, unusual_grbs)].mean()}')\n",
    "print(f'Mean duration is: {duration.mean()}')\n",
    "plt.hist(duration, bins = 80, range=(0, 100));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3857462b",
   "metadata": {},
   "source": [
    "Therefore, I was only able to identify a small number of short gamma-ray bursts. The following figure shows the distribution of burst durations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057eb3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "low, high = np.percentile(duration, [5, 95])\n",
    "duration = np.clip(duration, low, high)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sc = plt.scatter(features_2d[:, 0], features_2d[:, 1],\n",
    "                c=duration, cmap='seismic', alpha=0.7)\n",
    "plt.colorbar(label='Duration (seconds)')\n",
    "plt.title('GRBs colored by duration (blue=short, red=long)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e84126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following figure displays bursts with durations shorter than 2 seconds\n",
    "with h5py.File('./features.h5', 'r') as file:\n",
    "    short_grbs = file.attrs['short_grbs']\n",
    "\n",
    "y = np.isin(names, short_grbs)\n",
    "c = ['blue' if g else 'black' for g in y]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sc = plt.scatter(features_2d[:,0], features_2d[:,1], c=c, alpha=0.7)\n",
    "plt.title(f'Short GRBs with duration < 2s.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3be381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering of the 2D representation obtained from t-SNE using DBSCAN\n",
    "eps = 6.5\n",
    "min_samples = 20\n",
    "\n",
    "labels = dbscan(features_2d ,eps=eps, min_samples=min_samples)\n",
    "unique_labels = np.unique(labels)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sc = plt.scatter(features_2d[:,0], features_2d[:,1], c=labels, cmap='viridis', alpha=0.7)\n",
    "cbar = plt.colorbar(sc, ticks=unique_labels)\n",
    "cbar.set_label('Cluster ID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbed8e2",
   "metadata": {},
   "source": [
    "**Conclusions**.  \n",
    "The performed clustering analysis of gamma-ray bursts did not reveal well-defined structures, except for a weak separation into short and long bursts. This may be attributed to several factors:\n",
    "\n",
    "- **Coarse data selection** — the bursts selected in this study exhibit certain limitations, and a more careful data selection procedure should be applied.\n",
    "- **Initial feature set** — the extracted features may be insufficiently informative for identifying meaningful clusters. In future studies, it would be worthwhile to consider augmenting the scalogram-based features with additional parameters such as duration, spectral characteristics, luminosity, and redshift.\n",
    "- **Wavelet choice** — since the results strongly depend on the choice of the mother wavelet, a comparative analysis of different wavelet families (Daubechies, Haar, Morlet, etc.) should be conducted.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
